# -*- coding: utf-8 -*-
"""BlackCoffer_Project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YkUy4X9y3xUahnmPLZuu8hiZBiElaRKY
"""

import os
import gdown
import nltk
from nltk.tokenize import word_tokenize, sent_tokenize
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re

# Task 1: Functions for Data Extraction
# Function to extract article title and text
def extract_article(url):
    response = requests.get(url)
    soup = BeautifulSoup(response.content, 'html.parser')

    title = soup.find('h1').get_text().strip()
    article_content = soup.find('div', class_='td-post-content')
    paragraphs = article_content.find_all('p')
    article_text = ' '.join([p.get_text() for p in paragraphs])

    return title, article_text

# Function to save article to a file
def save_article(url_id, title, text):
    os.makedirs('articles', exist_ok=True)
    with open(f'articles/{url_id}.txt', 'w', encoding='utf-8') as file:
        file.write(title + '\n' + text)

# Download the NLTK punkt tokenizer models
nltk.download('punkt')

# Task 2: Download and load files from google drive
# Function to download files from Google Drive
def download_from_drive(link, output_dir):
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
    gdown.download_folder(link, output=output_dir, quiet=False)

# Function to load stop words from a directory
def load_stop_words(directory):
    stop_words = set()
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            file_path = os.path.join(directory, filename)
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                words = file.read().splitlines()
                stop_words.update(words)
    return stop_words

# Function to load master dictionary
def load_master_dictionary(directory):
    master_dict = {}
    for filename in os.listdir(directory):
        if filename.endswith('.txt'):
            file_path = os.path.join(directory, filename)
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:
                words = file.read().splitlines()
                dict_name = filename.split('.')[0]  # Use the filename without extension as the key
                master_dict[dict_name] = set(words)
    return master_dict

# Task 3: Text Analysis
# Function to clean text
def clean_text(text, stop_words):
    tokens = word_tokenize(text.lower())
    cleaned_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]
    return cleaned_tokens

# Function to count syllables in a word
def count_syllables(word):
    vowels = 'aeiou'
    count = 0
    if word[0] in vowels:
        count += 1
    for index in range(1, len(word)):
        if word[index] in vowels and word[index-1] not in vowels:
            count += 1
    if word.endswith('e'):
        count -= 1
    if word.endswith('le') and len(word) > 2 and word[-3] not in vowels:
        count += 1
    if count == 0:
        count += 1
    return count

# Function to calculate various metrics
def analyze_text(text, stop_words, positive_words, negative_words):
    cleaned_tokens = clean_text(text, stop_words)

    # Sentiment Analysis
    positive_score = sum(1 for token in cleaned_tokens if token in positive_words)
    negative_score = sum(1 for token in cleaned_tokens if token in negative_words)
    polarity_score = (positive_score - negative_score) / ((positive_score + negative_score) + 0.000001)
    subjectivity_score = (positive_score + negative_score) / (len(cleaned_tokens) + 0.000001)

    # Readability and Complexity Analysis
    sentences = sent_tokenize(text)
    avg_sentence_length = len(cleaned_tokens) / len(sentences)
    complex_words = [token for token in cleaned_tokens if count_syllables(token) > 2]
    percentage_complex_words = len(complex_words) / len(cleaned_tokens)
    fog_index = 0.4 * (avg_sentence_length + percentage_complex_words)

    # Other Metrics
    avg_words_per_sentence = len(cleaned_tokens) / len(sentences)
    complex_word_count = len(complex_words)
    word_count = len(cleaned_tokens)
    syllable_count = sum(count_syllables(word) for word in cleaned_tokens)
    avg_syllables_per_word = syllable_count / len(cleaned_tokens)
    personal_pronouns = len(re.findall(r'\b(I|we|my|ours|us)\b', text, re.I))
    avg_word_length = sum(len(word) for word in cleaned_tokens) / len(cleaned_tokens)

    return {
        'Positive Score': positive_score,
        'Negative Score': negative_score,
        'Polarity Score': polarity_score,
        'Subjectivity Score': subjectivity_score,
        'Avg Sentence Length': avg_sentence_length,
        'Percentage of Complex Words': percentage_complex_words,
        'Fog Index': fog_index,
        'Avg Number of Words Per Sentence': avg_words_per_sentence,
        'Complex Word Count': complex_word_count,
        'Word Count': word_count,
        'Syllable Per Word': avg_syllables_per_word,
        'Personal Pronouns': personal_pronouns,
        'Avg Word Length': avg_word_length
    }

# URLs for stopwords and master dictionary files
stopwords_drive_link = 'https://drive.google.com/drive/folders/1rd7YdoX8tED9mujc0c-6evJU4y7LFc_R?usp=sharing'
master_dict_drive_link = 'https://drive.google.com/drive/folders/1YRcVlJO3ZaC78iTC6JcunfZl7Fz4AL8v?usp=drive_link'

# Directories for downloaded files
stopwords_dir = 'stopwords_files'
master_dict_dir = 'master_dict_files'

# Download stopwords and master dictionary files
download_from_drive(stopwords_drive_link, stopwords_dir)
download_from_drive(master_dict_drive_link, master_dict_dir)

# Load stop words and master dictionaries
stop_words = load_stop_words(stopwords_dir)
master_dict = load_master_dictionary(master_dict_dir)

positive_words = master_dict.get('positive-words', set())
negative_words = master_dict.get('negative-words', set())

# Load the input Excel file
input_df = pd.read_excel('Input.xlsx')

# Create a DataFrame to store the results
output_columns = [
    'URL_ID', 'URL', 'Positive Score', 'Negative Score', 'Polarity Score',
    'Subjectivity Score', 'Avg Sentence Length', 'Percentage of Complex Words',
    'Fog Index', 'Avg Number of Words Per Sentence', 'Complex Word Count',
    'Word Count', 'Syllable Per Word', 'Personal Pronouns', 'Avg Word Length'
]
output_df = pd.DataFrame(columns=output_columns)

# Task 4: Executing the functions (Main function)
# Process each article
results = []
for index, row in input_df.iterrows():
    url_id = row['URL_ID']
    url = row['URL']
    try:
        # Extract and save the article text
        title, article_text = extract_article(url)
        save_article(url_id, title, article_text)
        print(f"Saved article {url_id}")

        # Analyze the text
        analysis_results = analyze_text(article_text, stop_words, positive_words, negative_words)

        # Append results to the output list
        result = {'URL_ID': url_id, 'URL': url}
        result.update(analysis_results)
        results.append(result)
        print(f"Processed article {url_id}")

    except Exception as e:
        print(f"Failed to process article {url_id}: {e}")

# Task 5: Saving the output file
# Convert results list to DataFrame
output_df = pd.DataFrame(results, columns=output_columns)

# Save the output to Excel
output_df.to_excel('Output Data Structure.xlsx', index=False)